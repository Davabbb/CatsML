{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 3\n",
    "### Выполнил: Бейлин Давид Михайлович"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройка Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# def download_contest3_data():\n",
    "#   # !mkdir ~/.kaggle # закоментить если ругается\n",
    "#   # !touch ~/.kaggle/kaggle.json # закоментить если ругается\n",
    "\n",
    "#   # токен надо сгенерировать в личном кабинете на kaggle (https://www.kaggle.com/settings/account)\n",
    "#   api_token = {\"username\":\"fokuspokus\",\"key\":\"abrakadabra\"}\n",
    "#   with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "#       json.dump(api_token, file)\n",
    "\n",
    "#   !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "#   !kaggle competitions download -p /content/drive/MyDrive/ -c ml-mipt-2023-contest-3\n",
    "\n",
    "#   if not os.path.isdir(\"/content/drive/MyDrive/contest3\"):\n",
    "#     !mkdir /content/drive/MyDrive/contest3\n",
    "\n",
    "#   !unzip /content/drive/MyDrive/ml-mipt-2023-contest-3.zip -d /content/drive/MyDrive/contest3\n",
    "\n",
    "# download_contest3_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (исследовательский анализ данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def draw(image, points):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for i in range(0, 18, 2):\n",
    "        x, y = points[i], points[i + 1]\n",
    "        circle = patches.Circle((x, y), radius=5, color='red')\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keypoints = pd.read_csv(\"data/train_labels.csv\")\n",
    "\n",
    "keypoints = keypoints.drop(index=140)\n",
    "keypoints = keypoints.drop(index=448)\n",
    "keypoints = keypoints.set_index('file_name')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing (подготовка данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsDataset(Dataset):\n",
    "    def __init__(self, folder, keypoint_data):\n",
    "        self.folder = folder\n",
    "        self.keypoint_data = keypoint_data.copy()\n",
    "        self.image_paths = [os.path.join(folder, filename) for filename in os.listdir(folder)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        scale_x = 256 / image.size[0]\n",
    "        scale_y = 256 / image.size[1]\n",
    "        image = image.resize((256, 256))\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "        image = transform(image)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        image = image.to(device)\n",
    "        file_name = os.path.basename(image_path)\n",
    "\n",
    "        points = self.keypoint_data.loc[file_name].values.copy()\n",
    "        for i in range(points.shape[0]):\n",
    "          if i % 2 == 0:\n",
    "            points[i] *= scale_x\n",
    "          else:\n",
    "            points[i] *= scale_y\n",
    "        points = torch.tensor(points, device=device)\n",
    "        return image.float(), points.float()\n",
    "\n",
    "\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.image_paths = [os.path.join(folder, filename) for filename in os.listdir(folder)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        scale_x = 256 / image.size[0]\n",
    "        scale_y = 256 / image.size[1]\n",
    "        image = image.resize((256, 256))\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        image = image.to(device)\n",
    "        file_name = os.path.basename(image_path)\n",
    "        return image.float(), file_name, scale_x, scale_y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Аугментация, чтобы лучше обучались перевернутые коты\n",
    "экспериментировал с кол-вом новых картинок, остановился на оптимальных чуть меньше 50% от датасета"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "def augmentation():\n",
    "    count_image = 4000\n",
    "    source_folder = \"/content/drive/MyDrive/contest3/images/images/train/\"\n",
    "    output_folder = \"/content/drive/MyDrive/contest3/images/images/train/\"\n",
    "    data_folder = \"/content/drive/MyDrive/contest3/train_labels.csv\"\n",
    "\n",
    "    image_files = os.listdir(source_folder)\n",
    "\n",
    "    selected_files = random.sample(image_files, count_image)\n",
    "\n",
    "    for filename in selected_files:\n",
    "        image_path = os.path.join(source_folder, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        rotated_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f'new_y_{filename}')\n",
    "        rotated_image.save(output_path)\n",
    "\n",
    "        coords = keypoints.loc[filename].copy()\n",
    "\n",
    "        data = [i for i in range(18)]\n",
    "        for i in range(18):\n",
    "            if i % 2 == 0:\n",
    "              data[i] = coords[i]\n",
    "            else:\n",
    "              data[i] = image.size[1] - coords[i]\n",
    "\n",
    "        data[0], data[1], data[2], data[3] = data[2], data[3], data[0], data[1]\n",
    "        data[6], data[7], data[12], data[13] = data[12], data[13], data[6], data[7]\n",
    "        data[8], data[9], data[14], data[15] = data[14], data[15], data[8], data[9]\n",
    "        data[10], data[11], data[16], data[17] = data[16], data[17], data[10], data[11]\n",
    "\n",
    "        keypoints.loc[f'new_y_{filename}'] = data\n",
    "\n",
    "        image.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset = CatsDataset(\"/content/drive/MyDrive/contest3/images/images/train/\", keypoints)\n",
    "test_dataset = TestDataset(\"/content/drive/MyDrive/contest3/images/images/test/\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "indices = np.arange(len(train_dataset))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(torch.utils.data.Subset(train_dataset, train_indices), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(torch.utils.data.Subset(train_dataset, val_indices), batch_size=32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе напишите функцию, принимающую модель, оптимизатор, кол-во эпох, и т.д, которая осуществляет обучение с заданными параметрами. Подумайте, что функция будет возвращать. \n",
    "\n",
    "Смысл этого раздела в том, чтобы не дублировать код обучения для каждого эксперимента. А еще на такую функцию легко накинуть перебор гиперпараметров... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, train_load, model, optimizer, criterion):\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in tqdm(train_load):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(x_val)\n",
    "                val_loss += criterion(val_outputs, y_val).item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию, которая бы делала предсказания. Функция принимает датасет/даталоадер и модель (мб еще что-то). Эта функция нужна вам, чтобы было удобнее считать метрику (по сути она будет склеивать предсказания из батчей в один массив). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    filenames = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, image_paths, scale_x, scale_y in tqdm(test_loader):\n",
    "            images = torch.tensor(images, dtype=torch.float32, device='cuda').clone().detach()\n",
    "            outputs = model(images)\n",
    "            predicted_keypoints = outputs.cpu().detach().numpy()\n",
    "\n",
    "            predicted_keypoints[:, 0::2] /= scale_x.unsqueeze(-1)\n",
    "            predicted_keypoints[:, 1::2] /= scale_y.unsqueeze(-1)\n",
    "\n",
    "            predictions.append(predicted_keypoints)\n",
    "            filenames += image_paths\n",
    "\n",
    "\n",
    "    predictions = np.vstack(predictions)\n",
    "    csv_columns = [\"file_names\", \"left_eye_x\", \"left_eye_y\", \"right_eye_x\", \"right_eye_y\", \"mouth_x\", \"mouth_y\", \"left_ear_1_x\", \"left_ear_1_y\", \"left_ear_2_x\", \"left_ear_2_y\", \"left_ear_3_x\", \"left_ear_3_y\", \"right_ear_1_x\", \"right_ear_1_y\", \"right_ear_2_x\", \"right_ear_2_y\", \"right_ear_3_x\", \"right_ear_3_y\"]\n",
    "    df = pd.DataFrame(predictions, columns=csv_columns[1:])\n",
    "    df['file_names'] = filenames\n",
    "\n",
    "    df.to_csv('test_preds.csv', index=False)\n",
    "\n",
    "    return predictions, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validation(model, val_load):\n",
    "    predictions, file_names = prediction(model, val_load)\n",
    "    total_metrics = 0.0\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        targets = torch.tensor(val_indices.loc[file_name].copy())\n",
    "        preds = torch.tensor(predictions[i])\n",
    "        mae = torch.mean(torch.abs(preds - targets))\n",
    "        total_metrics += mae.item()\n",
    "\n",
    "\n",
    "    for i in range(5):\n",
    "        file_path = os.path.join(\"drive/MyDrive/contest3/images/images/train\", file_names[i])\n",
    "        image = Image.open(file_path)\n",
    "        draw(image, predictions[i])\n",
    "\n",
    "    average_metrics = total_metrics / file_names.shape[0]\n",
    "    print(\"CMAE:\", average_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Эксперимент 1"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "первая попытка наугад"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.nn.functional.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 64 * 64)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.03\n",
    "model = KeypointCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses, val_losses = train(num_epochs, train_loader, model, optimizer, criterion)\n",
    "\n",
    "plt.plot(train_losses, label='Training Losses', color='blue')\n",
    "plt.plot(val_losses, label='Validation Losses', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "validation(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Эксперимент 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "понял что нужно больше эпох и другой лернинг рейт"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.nn.functional.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 64 * 64)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "model = KeypointCNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses, val_losses = train(num_epochs, train_loader, model, optimizer, criterion)\n",
    "\n",
    "plt.plot(train_losses, label='Training Losses', color='blue')\n",
    "plt.plot(val_losses, label='Validation Losses', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "validation(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Эксперимент 3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "добавил больше слоев и новый оптимизатор"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(nn.ReLU(inplace=True)(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu6(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_epochs = 25\n",
    "learning_rate = 0.001\n",
    "model = KeypointCNN()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses, val_losses = train(num_epochs, train_loader, model, optimizer, criterion)\n",
    "\n",
    "plt.plot(train_losses, label='Training Losses', color='blue')\n",
    "plt.plot(val_losses, label='Validation Losses', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "validation(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Эксперимент 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "добавил больше слоев, эпох и аугментацию"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KeypointCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(1024)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(1024 * 4 * 4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 18)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "        x = self.pool(F.relu(self.bn6(self.conv6(x))))\n",
    "        x = x.view(-1, 1024 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "model = KeypointCNN()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "augmentation()\n",
    "train_dataset = CatsDataset(\"/content/drive/MyDrive/contest3/images/images/train/\", keypoints)\n",
    "test_dataset = TestDataset(\"/content/drive/MyDrive/contest3/images/images/test/\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "indices = np.arange(len(train_dataset))\n",
    "train_indices, val_indices = train_test_split(indices, test_size=0.1, random_state=42)\n",
    "train_loader = DataLoader(torch.utils.data.Subset(train_dataset, train_indices), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(torch.utils.data.Subset(train_dataset, val_indices), batch_size=32, shuffle=False)\n",
    "\n",
    "train_losses, val_losses = train(num_epochs, train_loader, model, optimizer, criterion)\n",
    "\n",
    "plt.plot(train_losses, label='Training Losses', color='blue')\n",
    "plt.plot(val_losses, label='Validation Losses', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "validation(model, val_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ставил эксперементы с батч сайзом, чем больше тем лучше. На финальной попытке использовал 512, но ресурс закончиля\n",
    "также в первых попытках не делал ресайз"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (оценка качества модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion (Выводы)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Не успел сделать оптюну, чтобы разобраться с гипер параметрами\n",
    "Мало поработал с аугментацией\n",
    "Не понял как лучше стакать слои\n",
    "Получилось справиться с проблемой наклоненных и перевернутых котов, благодаря аугментации\n",
    "Получил маленькую ошибка на валидации"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
